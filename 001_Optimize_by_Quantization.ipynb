{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 137 ms (2022-08-21T02:57:21/2022-08-21T02:57:21)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-3b49e2b8-87f0-c515-798b-3492ec05a183)\r\n",
      "GPU 1: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-07628ed7-6ef8-fd67-7d03-cb6a89f72de4)\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "!nvidia-smi -L\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2' # No GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2.12 s (2022-08-21T02:57:21/2022-08-21T02:57:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Experiment tracking with mlflow\n",
    "import mlflow\n",
    "import mlflow.tensorflow as mltf\n",
    "from pathlib import Path\n",
    "import time, multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 34.7 ms (2022-08-21T02:57:23/2022-08-21T02:57:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.get_visible_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 231 ms (2022-08-21T02:57:23/2022-08-21T02:57:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "fmd = \"./mlflow/artifacts/1/38162c8d183043f1bfddf866e1ee9175/artifacts/model/data/model\" #final model directory\n",
    "\n",
    "\n",
    "targetMap='''aegypti landing\n",
    "aegypti smashed\n",
    "albopictus landing\n",
    "albopictus smashed\n",
    "Culex landing\n",
    "Culex smashed'''.split('\\n')\n",
    "\n",
    "test_path = \"./dataset/data_splitting/Pred/\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "\n",
    "gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test = gen.flow_from_directory(test_path, target_size=IMG_SIZE,\n",
    "                                      classes=targetMap, class_mode='categorical', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.88 ms (2022-08-21T02:57:24/2022-08-21T02:57:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dir_size(directory):\n",
    "    root_dir=Path(\".\")\n",
    "    size = sum(f.stat().st_size for f in root_dir.glob(directory+'/**/*') if f.is_file())\n",
    "    return f\"Size in MB: {size // (1024*1024)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 4.29 ms (2022-08-21T02:57:24/2022-08-21T02:57:24)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 56'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dir_size(fmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.09 s (2022-08-21T02:57:24/2022-08-21T02:57:25)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.load_model(fmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1 min 54 s (2022-08-21T02:57:25/2022-08-21T02:59:19)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 114s 1s/step - loss: 0.2003 - acc: 0.9467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20030760765075684, 0.9466666579246521]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, verbose=1) # CPU load, around 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Range quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 3.69 s (2022-08-21T02:59:19/2022-08-21T02:59:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(fmd) #from_keras_model\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "drq_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 28.3 ms (2022-08-21T02:59:23/2022-08-21T02:59:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 9'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drq_dir = Path(\"./optimized/drq\")\n",
    "drq_dir.mkdir(exist_ok=True, parents=True)\n",
    "drq_file = drq_dir/\"mosqueto_quant.tflite\"\n",
    "drq_file.write_bytes(drq_model);\n",
    "\n",
    "get_dir_size(\"./optimized/drq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 63.7 ms (2022-08-21T02:59:23/2022-08-21T02:59:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model, let see load time\n",
    "drq_model = tf.lite.Interpreter(model_path=\"./optimized/drq/mosqueto_quant.tflite\", \n",
    "                                num_threads=multiprocessing.cpu_count())\n",
    "\n",
    "input_index = drq_model.get_input_details()[0][\"index\"] \n",
    "output_index = drq_model.get_output_details()[0][\"index\"]\n",
    "\n",
    "# there is a problem, tflite by default inference 1 image at a time shape(1, 224, 224, 3)\n",
    "\n",
    "drq_model.resize_tensor_input(input_index, [BATCH_SIZE, 224, 224, 3]); \n",
    "drq_model.allocate_tensors() # from 1.22sec down to 40.1ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2.35 ms (2022-08-21T02:59:23/2022-08-21T02:59:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_2:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 32, 224, 224,   3], dtype=int32),\n",
       "  'shape_signature': array([ -1, 224, 224,   3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it can accept batch data\n",
    "drq_model.get_input_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a single-core predictor (By default)\n",
    "\n",
    "> CPU load, around 65% (All cores)  \n",
    "> the way you load data, can be a bottleneck  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 4 min 14 s (2022-08-21T02:59:23/2022-08-21T03:03:38)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add80e633838454c8078ab77963e667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "counter = 0\n",
    "for test_imgs, test_labels in tqdm(test):\n",
    "    if counter == 113: #3600 images / 32 batch_size = 112.5\n",
    "        break\n",
    "    if test_imgs.shape[0] == 16: # last batch is of size 16\n",
    "        test_imgs= np.concatenate((test_imgs, test_imgs))\n",
    "        test_labels= np.concatenate((test_labels, test_labels))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    drq_model.set_tensor(input_index, test_imgs)\n",
    "    \n",
    "    # Run inference\n",
    "    drq_model.invoke()\n",
    "    \n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = drq_model.tensor(output_index)\n",
    "    preds.append(np.argmax(output(), axis=1))\n",
    "    targets.append(np.argmax(test_labels, axis=1))\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 5.49 ms (2022-08-21T03:03:38/2022-08-21T03:03:38)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.9456'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[-1][16:] = 1\n",
    "targets[-1][16:] = 0\n",
    "f\"{np.sum(np.array(preds)==np.array(targets)) / test.n :.4f}\" # which is about 0.001 drop in accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't compare the time with model.evalute() cell (Why ?)\n",
    "\n",
    "[Why is TensorFlow Lite slower than TensorFlow on desktop?](https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 567 µs (2022-08-21T03:03:38/2022-08-21T03:03:38)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(5):\n",
    "        imgs, _ = test.next()\n",
    "        yield [imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 43.6 s (2022-08-21T03:03:38/2022-08-21T03:04:22)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(fmd)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# No float fallback some Edge devices can't handle float operations\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "fInt_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 39.5 ms (2022-08-21T03:04:22/2022-08-21T03:04:22)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 9'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fInt_dir = Path(\"./optimized/fInt\")\n",
    "fInt_dir.mkdir(exist_ok=True, parents=True)\n",
    "fInt_file = fInt_dir/\"mosqueto_quant.tflite\"\n",
    "fInt_file.write_bytes(fInt_model);\n",
    "\n",
    "get_dir_size(\"./optimized/fInt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.35 ms (2022-08-21T03:04:22/2022-08-21T03:04:22)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model, let see load time\n",
    "fInt_model = tf.lite.Interpreter(model_path=\"./optimized/fInt/mosqueto_quant.tflite\", \n",
    "                                num_threads=multiprocessing.cpu_count())\n",
    "\n",
    "input_index = fInt_model.get_input_details()[0][\"index\"] \n",
    "output_index = fInt_model.get_output_details()[0][\"index\"]\n",
    "\n",
    "# there is a problem, tflite by default inference 1 image at a time shape(1, 224, 224, 3)\n",
    "\n",
    "fInt_model.resize_tensor_input(input_index, [BATCH_SIZE, 224, 224, 3]); \n",
    "fInt_model.allocate_tensors() # from 1.22sec down to 4.1ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CPU load, around 80% (All cores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 3 min (2022-08-21T03:04:22/2022-08-21T03:07:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a74b8480814dca81973c1d639e57eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "counter = 0\n",
    "for test_imgs, test_labels in tqdm(test):\n",
    "    test_imgs = test_imgs.astype('int8')\n",
    "    test_labels = test_labels.astype('int8')\n",
    "    if counter == 113: #3600 images / 32 batch_size = 112.5\n",
    "        break\n",
    "    if test_imgs.shape[0] == 16: # last batch is of size 16\n",
    "        test_imgs= np.concatenate((test_imgs, test_imgs))\n",
    "        test_labels= np.concatenate((test_labels, test_labels))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    fInt_model.set_tensor(input_index, test_imgs)\n",
    "    \n",
    "    # Run inference\n",
    "    fInt_model.invoke()\n",
    "    \n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = fInt_model.tensor(output_index)\n",
    "    preds.append(np.argmax(output(), axis=1))\n",
    "    targets.append(np.argmax(test_labels, axis=1))\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 3.93 ms (2022-08-21T03:07:23/2022-08-21T03:07:23)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.8600'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[-1][16:] = 1\n",
    "targets[-1][16:] = 0\n",
    "f\"{np.sum(np.array(preds)==np.array(targets)) / test.n :.4f}\" # which is about 0.8 drop in accuracy \n",
    "# (previous try was less than 4%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Float 16 :\n",
    "\n",
    "```\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.9",
   "language": "python",
   "name": "tf2.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 150 ms (2022-08-23T05:26:21/2022-08-23T05:26:22)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-3b49e2b8-87f0-c515-798b-3492ec05a183)\r\n",
      "GPU 1: NVIDIA GeForce GTX 1080 Ti (UUID: GPU-07628ed7-6ef8-fd67-7d03-cb6a89f72de4)\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "!nvidia-smi -L\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2.74 s (2022-08-23T05:26:22/2022-08-23T05:26:25)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools, glob\n",
    "\n",
    "# Experiment tracking with mlflow\n",
    "import mlflow\n",
    "import mlflow.tensorflow as mltf\n",
    "from pathlib import Path\n",
    "import time, multiprocessing, tarfile\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 35.4 ms (2022-08-23T05:26:25/2022-08-23T05:26:25)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.get_visible_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 942 µs (2022-08-23T05:26:25/2022-08-23T05:26:25)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fmd = \"./mlflow/artifacts/1/38162c8d183043f1bfddf866e1ee9175/artifacts/model/data/model\" #final model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 656 µs (2022-08-23T05:26:25/2022-08-23T05:26:25)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dir_size(directory):\n",
    "    root_dir=Path(\".\")\n",
    "    size = sum(f.stat().st_size for f in root_dir.glob(directory+'/**/*') if f.is_file())\n",
    "    return f\"Size in MB: {size // (1024*1024)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 5.93 ms (2022-08-23T05:26:26/2022-08-23T05:26:26)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 56'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dir_size(fmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 216 ms (2022-08-23T05:26:26/2022-08-23T05:26:26)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"mosquito\")\n",
    "mltf.autolog()\n",
    "\n",
    "mlflow.set_tags({\"Pretrain Model\": \"Saved Model\", \n",
    "                \"Preprocessing\" : \"Keras VGG16 Preprocessing\",\n",
    "                \"Pretrained Used Layers\" : \"first 13 Layers - Block4\",\n",
    "                \"Framework\": \"tensorflow.keras\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.71 ms (2022-08-23T05:26:26/2022-08-23T05:26:26)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"./dataset/data_splitting/Train/\"\n",
    "valid_path = \"./dataset/data_splitting/Test/\"\n",
    "test_path = \"./dataset/data_splitting/Pred/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.31 ms (2022-08-23T05:26:26/2022-08-23T05:26:26)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can add more augmentations, if you want\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rotation_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 42.8 ms (2022-08-23T05:26:26/2022-08-23T05:26:27)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.log_params({\"train ImageDataGenerator\": {\"rotation_range\": 0.2,\n",
    "                                                \"horizontal_flip\": True,\n",
    "                                                \"vertical_flip\": True,\n",
    "                                                \"preprocessing_function\": preprocess_input},\n",
    "                   \"test and valid ImageDataGenerator\":{\"preprocessing_function\": preprocess_input}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 4.48 ms (2022-08-23T05:26:27/2022-08-23T05:26:27)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['aegypti landing',\n",
       " 'aegypti smashed',\n",
       " 'albopictus landing',\n",
       " 'albopictus smashed',\n",
       " 'Culex landing',\n",
       " 'Culex smashed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetMap='''aegypti landing\n",
    "aegypti smashed\n",
    "albopictus landing\n",
    "albopictus smashed\n",
    "Culex landing\n",
    "Culex smashed'''.split('\\n')\n",
    "targetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.51 ms (2022-08-23T05:26:27/2022-08-23T05:26:27)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "NUM_CLASSES = len(targetMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 29.5 ms (2022-08-23T05:26:28/2022-08-23T05:26:28)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.log_params({\"IMG_SIZE\":IMG_SIZE, \"Format\":\"RGB\", \"BATCH_SIZE\": BATCH_SIZE, \"EPOCHS\": EPOCHS,\n",
    "                   \"NUM_CLASSES\": len(targetMap)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 684 ms (2022-08-23T05:26:29/2022-08-23T05:26:30)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 images belonging to 6 classes.\n",
      "Found 1799 images belonging to 6 classes.\n",
      "Found 3600 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory(train_path, target_size=IMG_SIZE,\n",
    "                                      classes=targetMap, class_mode='categorical', batch_size=BATCH_SIZE)\n",
    "valid = gen.flow_from_directory(valid_path, target_size=IMG_SIZE,\n",
    "                                      classes=targetMap, class_mode='categorical', batch_size=BATCH_SIZE)\n",
    "test = gen.flow_from_directory(test_path, target_size=IMG_SIZE,\n",
    "                                      classes=targetMap, class_mode='categorical', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2.05 s (2022-08-23T05:26:30/2022-08-23T05:26:32)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7f19bcad1520>,\n",
       " <keras.layers.preprocessing.image_preprocessing.Rescaling at 0x7f1a9879d460>,\n",
       " <keras.engine.functional.Functional at 0x7f1a9879d370>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f19bcad1d30>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f19bcafbd60>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f19f739adc0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f19f739ac70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f19bcae02b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f19bcae07c0>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f19bcae0a90>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x7f1a41f8fdc0>,\n",
       " <keras.layers.reshaping.flatten.Flatten at 0x7f1a41f8f850>,\n",
       " <keras.layers.core.dense.Dense at 0x7f1a41f8f670>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x7f1a41f8f370>,\n",
       " <keras.layers.core.dense.Dense at 0x7f19f73e11c0>,\n",
       " <keras.layers.regularization.dropout.Dropout at 0x7f19f73e14f0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f19f73e1730>,\n",
       " <keras.layers.core.dense.Dense at 0x7f19f73e1c70>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = keras.models.load_model(fmd)\n",
    "base_model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But there are some problems \n",
    "1. the Rescaling layer is not supported for pruning.\n",
    "2. Functional layer (the way we used vgg model) is not supported as well\n",
    "\n",
    "#### Also there are some recommendations :\n",
    "* It's generally better to finetune with pruning as opposed to training from scratch.\n",
    "* Try pruning the later layers instead of the first layers.\n",
    "* Avoid pruning critical layers (e.g. attention mechanism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 359 µs (2022-08-23T05:26:32/2022-08-23T05:26:32)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def prune_dense(layer):\n",
    "#     if isinstance(layer, tf.keras.layers.Dense):\n",
    "#         return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
    "#     return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 668 µs (2022-08-23T05:26:32/2022-08-23T05:26:32)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prune_custom_layer(layer):\n",
    "    # prunning_params is optional (don't use it if you want)\n",
    "#     end_step = np.ceil(train.n/BATCH_SIZE).astype(np.int32) * EPOCHS\n",
    "#     prunning_params = {\n",
    "#         tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.5, \n",
    "#                                              final_sparsity=0.9,\n",
    "#                                              begin_step=0,\n",
    "#                                             end_step=end_step)\n",
    "#     }\n",
    "    try:\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **prunning_params)\n",
    "    except:\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 142 ms (2022-08-23T05:26:33/2022-08-23T05:26:33)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 28, 28, 512)       7635264   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 512)      2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 256)       1179904   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 26, 26, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 256)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 11, 11, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               819456    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,974,790\n",
      "Trainable params: 2,337,734\n",
      "Non-trainable params: 7,637,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function=prune_custom_layer\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 14 ms (2022-08-23T05:26:35/2022-08-23T05:26:35)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=CategoricalCrossentropy(from_logits=True), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2 min 2 s (2022-08-23T05:26:36/2022-08-23T05:28:38)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "132/132 [==============================] - 42s 290ms/step - loss: 0.1975 - acc: 0.9405 - val_loss: 0.4933 - val_acc: 0.8810\n",
      "Epoch 2/3\n",
      "132/132 [==============================] - 36s 270ms/step - loss: 0.1587 - acc: 0.9524 - val_loss: 0.2453 - val_acc: 0.9389\n",
      "Epoch 3/3\n",
      "132/132 [==============================] - 35s 268ms/step - loss: 0.1130 - acc: 0.9626 - val_loss: 0.1594 - val_acc: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmped8v6t4w/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmped8v6t4w/model/data/model/assets\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "# use tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir) to log metrics on Tensorboard\n",
    "\n",
    "h = model.fit(train, validation_data=valid, callbacks=callbacks, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 6.56 s (2022-08-23T05:28:38/2022-08-23T05:28:44)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 6s 56ms/step - loss: 0.1163 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11631447076797485, 0.9700000286102295]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you may see around around 2% gain in test accuracy \n",
    "# (not the case for well trained base_model)\n",
    "\n",
    "model.evaluate(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 5.23 ms (2022-08-23T05:28:44/2022-08-23T05:28:44)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 56'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfmd = f\"./mlflow/artifacts/1/{mlflow.active_run().info.run_id}/artifacts/model/data/model\"\n",
    "get_dir_size(pfmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait, what just happened ?\n",
    "\n",
    "__Why our pruned model is bigger than base model ?__  \n",
    "* _Short answer that's not the correct way to save a prunned model_\n",
    "* I see this exact problem alot in the Industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Both `tfmot.sparsity.keras.strip_pruning` and applying a standard compression algorithm `(e.g. via gzip)` are necessary to see the compression benefits of pruning.*  \n",
    "    * Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning. However, pruning makes most of the weights zeros, which is added redundancy that algorithms can utilize to further compress the model. \n",
    "\n",
    "* __`strip_pruning` is necessary since it removes every `tf.Variable` that pruning only needs during training, which would otherwise add to model size during inference__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 135 ms (2022-08-23T05:28:52/2022-08-23T05:28:52)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_to_export = tfmot.sparsity.keras.strip_pruning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 2.6 s (2022-08-23T05:28:52/2022-08-23T05:28:55)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./optimized/pruned_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./optimized/pruned_model/assets\n"
     ]
    }
   ],
   "source": [
    "keras.models.save_model(model_to_export, \"./optimized/pruned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 6.44 ms (2022-08-23T05:28:55/2022-08-23T05:28:55)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 38'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dir_size(\"./optimized/pruned_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 899 ms (2022-08-23T05:28:55/2022-08-23T05:28:56)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"./optimized/pruned_model/\", compile=False)\n",
    "# For evaluation, you must run compile again (but don't need it for production as you only use predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 15 s (2022-08-23T05:28:56/2022-08-23T05:29:11)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    imgs, _ = test.next()\n",
    "    for img in imgs:\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        model.predict(img, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As recommended by TF let's use gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 719 µs (2022-08-23T05:29:11/2022-08-23T05:29:11)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gzipit(input_dir, output_file):\n",
    "    root_dir=Path(\".\")\n",
    "    with tarfile.open(output_file+\".tgz\", \"w:gz\") as tar:\n",
    "        for f in root_dir.glob(input_dir+'/*'):\n",
    "            tar.add(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 1.67 s (2022-08-23T05:29:11/2022-08-23T05:29:13)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Size in MB: 35'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"./optimized/pruned_gziped/\", exist_ok=True)\n",
    "gzipit(\"./optimized/pruned_model\", \"./optimized/pruned_gziped/model\")\n",
    "get_dir_size(\"./optimized/pruned_gziped/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let use Dynamic Range quantization on top of pruned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 6.64 s (2022-08-23T05:29:13/2022-08-23T05:29:19)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpiddhthaz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpiddhthaz/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_to_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "drq_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 413 ms (2022-08-23T05:29:19/2022-08-23T05:29:20)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"./optimized/pruned_drq/mosqueto_quant.tflite\"\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "    f.write(drq_model)\n",
    "\n",
    "os.makedirs(\"./optimized/pruned_gziped_tflite/\", exist_ok=True)\n",
    "gzipit(\"./optimized/pruned_drq/\", \"./optimized/pruned_gziped_tflite/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>✔️ 6.17 ms (2022-08-23T05:29:20/2022-08-23T05:29:20)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No optimization model: Size in MB: 56 only quantized: 9 MB\n",
      "gzip pruned: Size in MB: 35\n",
      "Pruned and quantized gzipped: Size in MB: 7\n",
      "\n",
      "More than 8x saved\n"
     ]
    }
   ],
   "source": [
    "s1 = get_dir_size(fmd)\n",
    "s2 = get_dir_size(\"./optimized/pruned_gziped\")\n",
    "s3 = get_dir_size(\"./optimized/pruned_gziped_tflite\")\n",
    "\n",
    "print(f\"No optimization model: {s1} only quantized: 9 MB\")    \n",
    "print(f\"gzip pruned: {s2}\")\n",
    "print(f\"Pruned and quantized gzipped: {s3}\")\n",
    "fraq = int(s1.split(\" \")[-1])//int(s3.split(\" \")[-1])\n",
    "print(f\"\\nMore than {fraq}x saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.9",
   "language": "python",
   "name": "tf2.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
